{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha2048 - Training in Google Colab\n",
    "\n",
    "This notebook demonstrates how to train a DQN agent to play 2048 using the alpha2048 package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install the Package\n",
    "\n",
    "Install directly from GitHub repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/nnaakkaaii/alpha2048.git#subdirectory=python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import alpha2048 components\n",
    "from pkg.environments.game_2048_env import Game2048Env\n",
    "from pkg.agents.dqn_agent import DQNAgent\n",
    "from pkg.networks.dqn import DQN\n",
    "from pkg.networks.cnn import CNN\n",
    "from pkg.utils.replay_memory import ReplayMemory\n",
    "from pkg.utils.state import get_state_flatten, get_state_one_hot, get_state_cnn\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "config = {\n",
    "    'episodes': 1000,           # Number of training episodes\n",
    "    'batch_size': 64,           # Batch size for training\n",
    "    'lr': 1e-4,                 # Learning rate\n",
    "    'gamma': 0.99,              # Discount factor\n",
    "    'epsilon_start': 1.0,       # Starting exploration rate\n",
    "    'epsilon_end': 0.01,        # Minimum exploration rate\n",
    "    'epsilon_decay': 0.995,     # Exploration decay rate\n",
    "    'target_update': 10,        # Target network update frequency\n",
    "    'memory_size': 10000,       # Experience replay buffer size\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'network_type': 'cnn',      # 'mlp' or 'cnn'\n",
    "    'use_double_dqn': True,     # Use Double DQN\n",
    "}\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = Game2048Env()\n",
    "\n",
    "# Determine state size based on network type\n",
    "if config['network_type'] == 'cnn':\n",
    "    state_size = (16, 4, 4)  # CNN expects (channels, height, width)\n",
    "    get_state_fn = get_state_cnn\n",
    "else:\n",
    "    state_size = 256  # One-hot encoding: 16 positions × 16 possible values\n",
    "    get_state_fn = get_state_one_hot\n",
    "\n",
    "# Create agent\n",
    "agent = DQNAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=4,\n",
    "    lr=config['lr'],\n",
    "    gamma=config['gamma'],\n",
    "    epsilon=config['epsilon_start'],\n",
    "    epsilon_min=config['epsilon_end'],\n",
    "    epsilon_decay=config['epsilon_decay'],\n",
    "    memory_size=config['memory_size'],\n",
    "    batch_size=config['batch_size'],\n",
    "    target_update=config['target_update'],\n",
    "    device=config['device'],\n",
    "    network_type=config['network_type'],\n",
    "    use_double_dqn=config['use_double_dqn']\n",
    ")\n",
    "\n",
    "print(f\"Agent created with {config['network_type'].upper()} network\")\n",
    "print(f\"State size: {state_size}\")\n",
    "print(f\"Action size: 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training metrics\n",
    "scores = []\n",
    "max_tiles = []\n",
    "epsilon_values = []\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "for episode in range(config['episodes']):\n",
    "    state = env.reset()\n",
    "    state = get_state_fn(state)\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Choose action\n",
    "        action = agent.act(state)\n",
    "        \n",
    "        # Take action in environment\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = get_state_fn(next_state)\n",
    "        \n",
    "        # Store experience\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Update state\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Train agent\n",
    "        if len(agent.memory) > config['batch_size']:\n",
    "            loss = agent.replay()\n",
    "            if loss is not None:\n",
    "                losses.append(loss)\n",
    "    \n",
    "    # Update target network\n",
    "    if episode % config['target_update'] == 0:\n",
    "        agent.update_target_network()\n",
    "    \n",
    "    # Update epsilon\n",
    "    agent.update_epsilon()\n",
    "    \n",
    "    # Record metrics\n",
    "    scores.append(info['score'])\n",
    "    max_tiles.append(info['max_tile'])\n",
    "    epsilon_values.append(agent.epsilon)\n",
    "    \n",
    "    # Print progress\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        avg_score = np.mean(scores[-10:])\n",
    "        avg_max_tile = np.mean(max_tiles[-10:])\n",
    "        print(f\"Episode {episode + 1}/{config['episodes']}\")\n",
    "        print(f\"  Avg Score: {avg_score:.0f}\")\n",
    "        print(f\"  Avg Max Tile: {avg_max_tile:.0f}\")\n",
    "        print(f\"  Epsilon: {agent.epsilon:.3f}\")\n",
    "        if losses:\n",
    "            print(f\"  Avg Loss: {np.mean(losses[-100:]):.4f}\")\n",
    "        print()\n",
    "        \n",
    "    # Plot progress every 100 episodes\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        \n",
    "        # Plot scores\n",
    "        axes[0, 0].plot(scores)\n",
    "        axes[0, 0].set_title('Game Scores')\n",
    "        axes[0, 0].set_xlabel('Episode')\n",
    "        axes[0, 0].set_ylabel('Score')\n",
    "        \n",
    "        # Plot max tiles\n",
    "        axes[0, 1].plot(max_tiles)\n",
    "        axes[0, 1].set_title('Maximum Tiles')\n",
    "        axes[0, 1].set_xlabel('Episode')\n",
    "        axes[0, 1].set_ylabel('Max Tile')\n",
    "        \n",
    "        # Plot epsilon\n",
    "        axes[1, 0].plot(epsilon_values)\n",
    "        axes[1, 0].set_title('Exploration Rate (Epsilon)')\n",
    "        axes[1, 0].set_xlabel('Episode')\n",
    "        axes[1, 0].set_ylabel('Epsilon')\n",
    "        \n",
    "        # Plot loss\n",
    "        if losses:\n",
    "            axes[1, 1].plot(losses[-1000:])  # Plot last 1000 losses\n",
    "            axes[1, 1].set_title('Training Loss (Last 1000)')\n",
    "            axes[1, 1].set_xlabel('Training Step')\n",
    "            axes[1, 1].set_ylabel('Loss')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final average score (last 100 episodes): {np.mean(scores[-100:]):.0f}\")\n",
    "print(f\"Final average max tile (last 100 episodes): {np.mean(max_tiles[-100:]):.0f}\")\n",
    "print(f\"Best score achieved: {max(scores):.0f}\")\n",
    "print(f\"Best tile achieved: {max(max_tiles):.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_path = 'alpha2048_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': agent.q_network.state_dict(),\n",
    "    'optimizer_state_dict': agent.optimizer.state_dict(),\n",
    "    'epsilon': agent.epsilon,\n",
    "    'config': config,\n",
    "    'scores': scores,\n",
    "    'max_tiles': max_tiles,\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Download the model (for Colab)\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(model_path)\n",
    "    print(\"Model downloaded!\")\n",
    "except ImportError:\n",
    "    print(\"Not running in Colab, model saved locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test the Trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, env, num_games=10, visualize=True):\n",
    "    \"\"\"Test the trained agent.\"\"\"\n",
    "    test_scores = []\n",
    "    test_max_tiles = []\n",
    "    \n",
    "    # Set agent to evaluation mode (no exploration)\n",
    "    original_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0\n",
    "    \n",
    "    for game in range(num_games):\n",
    "        state = env.reset()\n",
    "        state = get_state_fn(state)\n",
    "        done = False\n",
    "        \n",
    "        if visualize and game == 0:  # Visualize first game\n",
    "            print(f\"\\nGame {game + 1}:\")\n",
    "            print(\"Initial board:\")\n",
    "            env.render()\n",
    "        \n",
    "        step = 0\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = get_state_fn(next_state)\n",
    "            state = next_state\n",
    "            step += 1\n",
    "            \n",
    "            if visualize and game == 0 and (step % 50 == 0 or done):\n",
    "                print(f\"\\nStep {step}, Action: {['Up', 'Down', 'Left', 'Right'][action]}\")\n",
    "                print(f\"Score: {info['score']}, Max Tile: {info['max_tile']}\")\n",
    "                if not done:\n",
    "                    env.render()\n",
    "        \n",
    "        test_scores.append(info['score'])\n",
    "        test_max_tiles.append(info['max_tile'])\n",
    "        \n",
    "        if visualize and game == 0:\n",
    "            print(\"\\nFinal board:\")\n",
    "            env.render()\n",
    "        \n",
    "        print(f\"Game {game + 1}: Score = {info['score']}, Max Tile = {info['max_tile']}\")\n",
    "    \n",
    "    # Restore original epsilon\n",
    "    agent.epsilon = original_epsilon\n",
    "    \n",
    "    print(f\"\\nTest Results ({num_games} games):\")\n",
    "    print(f\"Average Score: {np.mean(test_scores):.0f} ± {np.std(test_scores):.0f}\")\n",
    "    print(f\"Average Max Tile: {np.mean(test_max_tiles):.0f} ± {np.std(test_max_tiles):.0f}\")\n",
    "    print(f\"Best Score: {max(test_scores)}\")\n",
    "    print(f\"Best Tile: {max(test_max_tiles)}\")\n",
    "    \n",
    "    return test_scores, test_max_tiles\n",
    "\n",
    "# Test the agent\n",
    "test_scores, test_max_tiles = test_agent(agent, env, num_games=10, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Test Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Test scores distribution\n",
    "axes[0].bar(range(len(test_scores)), test_scores)\n",
    "axes[0].axhline(y=np.mean(test_scores), color='r', linestyle='--', label=f'Mean: {np.mean(test_scores):.0f}')\n",
    "axes[0].set_title('Test Game Scores')\n",
    "axes[0].set_xlabel('Game')\n",
    "axes[0].set_ylabel('Score')\n",
    "axes[0].legend()\n",
    "\n",
    "# Max tiles distribution\n",
    "unique_tiles, counts = np.unique(test_max_tiles, return_counts=True)\n",
    "axes[1].bar(unique_tiles, counts)\n",
    "axes[1].set_title('Maximum Tile Distribution')\n",
    "axes[1].set_xlabel('Max Tile')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xscale('log', base=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print tile achievement statistics\n",
    "print(\"\\nTile Achievement Statistics:\")\n",
    "for tile in [256, 512, 1024, 2048, 4096]:\n",
    "    count = sum(1 for t in test_max_tiles if t >= tile)\n",
    "    percentage = (count / len(test_max_tiles)) * 100\n",
    "    print(f\"  Reached {tile}: {count}/{len(test_max_tiles)} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load and Continue Training (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a saved model to continue training\n",
    "def load_model(agent, model_path):\n",
    "    \"\"\"Load a saved model.\"\"\"\n",
    "    checkpoint = torch.load(model_path, map_location=agent.device)\n",
    "    agent.q_network.load_state_dict(checkpoint['model_state_dict'])\n",
    "    agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    agent.epsilon = checkpoint['epsilon']\n",
    "    print(f\"Model loaded from {model_path}\")\n",
    "    print(f\"Epsilon restored to: {agent.epsilon:.3f}\")\n",
    "    return checkpoint.get('scores', []), checkpoint.get('max_tiles', [])\n",
    "\n",
    "# Example: Load and continue training\n",
    "# scores, max_tiles = load_model(agent, 'alpha2048_model.pth')\n",
    "# Continue training from here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Tuning Tips\n",
    "\n",
    "- **Learning Rate**: Start with 1e-4, adjust based on loss convergence\n",
    "- **Batch Size**: Larger batches (64-128) for more stable training\n",
    "- **Epsilon Decay**: Slower decay (0.995-0.999) for better exploration\n",
    "- **Network Type**: CNN generally performs better for 2048\n",
    "- **Memory Size**: Larger buffer (10000+) for more diverse experiences\n",
    "- **Target Update**: More frequent updates (5-10) for faster learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}